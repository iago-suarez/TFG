\chapter{Funcionalidades Destacadas}


\section{Control de Usuarios}
	É preciso un mínimo control de usuarios para controlar que sube vídeos á plataforma.

\section{Carga de Vídeo}
	Dado que a aplicación traballará con vídeos subidos polos usuarios, o primeiro paso é lograr
	a subida exitosa de vídeos á plataforma. Para este fin empregarase un formulario HTML que 
	viaxa sobre unha chamada POST de HTTP. Cando o navegador faga esta chamada incluíndo o vídeo
	como parte do formulario, este vídeo comezará a subirse ao servidor en pequenos anaquiños 
	(data chunk).
	
	É de especial importancia que no caso de que o vídeo teña un peso considerable e precise 
	duns cantos segundos para subirse á plataforma, o usuario poida coñecer de forma gráfica
	o avance deste proceso.
	
	Con este fin, crease un sistema de notificación de progreso baseado no 
	django-progressbarupload \cite{django-progressbarupload}, este sistema apoiase nunha compoñente
	fundamental chamada VideoUploadHandler, que é unha extensión da interface de Django 
	TemporaryFileUploadHandler \cite{TemporaryFileUploadHandler}, e que basicamente manexa a subida
	dun ficheiro de tamaño considerable.\\
	
	Esta compoñente componse dunha función de inicio (handle\_raw\_input) que crea unha entrada 
	na Cache de Django, almacenando como chave un número aleatorio e a IP do cliente que está a
	subir o vídeo, e como valor o tamaño do ficheiro e o porcentaxe de este que xa está subido.
	Esta entrada será actualizada cada vez que o servidor reciba un novo anaco de vídeo (mediante
	a outra función do compoñente VideoUploadHandler, receive\_data\_chunk). 
	
	Por outra parte, para que visualmente o cliente poida ver o avance da subida a través unha
	barra de progreso, crease unha función asíncrona en javascript (Tecnoloxía AJAX), que 
	periodicamente consulta ao servidor para obter o valor da cache que indica a porcentaxe de
	subida do vídeo, e unha vez obtido, actualiza a barra de progreso para mostralo. Todo isto
	ten lugar no navegador mentres este inda está a subir o arquivo de vídeo.

	Unha vez que a subida se completa, o POST é manexado pola vista UploadView, que se todos
	os datos do formulario son correctos, encargase de crear un modelo VideoModel. Como parte
	desta creación o vídeo pasa do directorio temporal no que foi almacenado (baixo linux por 
	defecto é /tmp) a un directorio calculado pola función get\_valid\_filename. Esta función
	pásaselle ao modelo como parte do seu campo ''video'' do tipo FileField.
	
	\begin{figure}[htp]
	\begin{center}
		\includegraphics[scale=0.3]{figures/SubidaVideo.png}
		\caption{Diagrama de secuencia do proceso seguido cando se fai o Submit do Formulario 
		de creación de vídeos}
	\label{fig:SubidaVideo}
	\end{center}
	\end{figure}
	
	Unha vez subido o vídeo satisfactoriamente xa está asentada a base da nosa aplicación. Con esta
	parte completada a seguinte funcionalidade a atender será a de mostrar todos os vídeos 
	dispoñibles na plataforma.

\section{Lista de vídeos e Imaxe de Portada}
    Co fin de que os usuarios accedan aos distintos vídeos subidos, deseñase unha páxina web que será a
    principal do módulo video\_manager na cal un usuario pode visualizar unha \textbf{ lista paxinada dos
    vídeos} dispoñibles. Esta lista estará ordenada comezando polo vídeo mais recente e rematando
    polo mais antigo, tamén se contempla a posibilidade de poder filtralos por exemplo polo nome.
    
    Para a elaboración de esta páxina empreganse o elementos dos que dispón Django como son a
    clase ListView co seu atributo paginate\_by e o filtrado de resultado co método 
    QuerySet.filter(...), mentres que para o paso das palabras claves polas que buscar un vídeo
    empregase un parámetro de URL chamado 'name'.\\
    
    Outra funcionalidade interesante de cara a amosar unha lista de vídeos é a de poder mostrar unha
    imaxe representativa de cada un deles. Con este fin crease a vista SuccessfulUpload, á que se
    redirecciona unha vez o vídeo é subido correctamente para seleccionar a súa \textbf{imaxe de portada}.
    
    \begin{figure}[htp]
    \begin{center}
        \includegraphics[scale=0.35]{figures/SuccessfulUploadScreen.png}
        \caption{Captura de pantalla da páxina web SuccessfulUpload}
    \label{fig:SuccessfulUploadScreen}
    \end{center}
    \end{figure}
    
    Con este fin extráense mediante ffmpeg unha serie de fotogramas do vídeo, que se gardan nun
    directorio temporal para que unha vez o vídeo estea subido e analizado, as imaxes se integren 
    como parte dun formulario na páxina SuccessfulUpload. Mediante este formulario, xerado co plugin
    image-picker\cite{ImagePickerPage}, o usuario poderá escoller o fotograma que lle pareza mais
    representativo do vídeo e unha vez que pulse no botón de ''Submit'' este fotograma gardarase
    como parte do Modelo de Django VideoModel, quedando pois accesible para que a lista de vídeos 
    poida amosalo.
    
	
\section{Reprodución de Vídeo}
    Desexase que a aplicación permita a reprodución dos vídeos contidos, mediante técnicas de
    streaming ou pseudo-streaming. Neste caso empregarase o pseudo-streaming polo sinxela que
    resulta esta implementación empregando as capacidades da etiqueta $<video>$ de HTML5 en conxunto
    con un servidor HTTP como Apache ou o servidor para desenvolvemento de Django.

    \begin{figure}[htp]
    \begin{center}
        \includegraphics[scale=0.55]{figures/VideoTagHtml5.png}
        \caption{tag en html5, coas súas fontes e coas capas $<canvas>$ asociadas}
    \label{fig:VideoTagHtml5}
    \end{center}
    \end{figure}
    
    na figura \ref{fig:VideoTagHtml5} podemos ver o resultado en HTML5, vense claramente a etiqueta
    $<video>$ coas súas fontes de datos $<source>$, cabe destacar que aquí engadiuse o atributo fps
    (Fotogramas Por Segundo do inglés Frames per second) que non pertence ao estándar definido pola 
    W3C\ref{w3schools-source-tag} mais no caso da nosa aplicación é fundamental para poder coñecer 
    a velocidade á que o navegador vai amosar os fotogramas do vídeo.
    
    Outra cuestión a aclarar é o motivo polo cal non se subministra a fonte de vídeo en formato
    .ogv que a W3C recomenda. A resposta é que o codec theora que ffmpeg inclue soporta 
    decodificación pero NON codificación, polo cal é posible pasar de vídeos en .ogv a outros 
    formatos pero non de outros formatos a .ogv imposibilitando pois que se poida ofrecer o 
    vídeo neste formato mentres o codec de ffmpeg non o permita. Non obstante, isto non supón un 
    problema, xa que como se pode ver na táboa seguinte todos os navegadores permiten a reprodución
    baseándose nestes dous formatos:
    
    % Please add the following required packages to your document preamble:
    % If you use beamer only pass "xcolor=table" option, i.e. \documentclass[xcolor=table]{beamer}
    \begin{table}[]
        \centering
        \label{supported-video-formats}
        \begin{tabular}{llll}
         \hline
            \multicolumn{1}{c}{\bfseries Navegador} & 
            \multicolumn{1}{c}{\bfseries MP4} & 
            \multicolumn{1}{c}{\bfseries WebM} & 
            \multicolumn{1}{c}{\bfseries Ogg} \\

            \rowcolor[HTML]{EFEFEF} 
            Internet Explorer & SI                                                                                              & NON & NON \\
            Chrome            & SI                                                                                              & SI  & SI  \\
            \rowcolor[HTML]{EFEFEF} 
            Firefox           & \begin{tabular}[c]{@{}l@{}}SI \\ dende Firefox 21 (win)\\ dende Firefox 30 (linux)\end{tabular} & SI  & SI  \\
            Safari            & SI                                                                                              & NON & NON \\
            \rowcolor[HTML]{EFEFEF} 
            Opera             & \begin{tabular}[c]{@{}l@{}}SI\\ dende Opera 25\end{tabular}                                     & SI  & SI 
        \end{tabular}
        \caption{Táboa de formatos de vídeo soportados polos distintos navegadores}
    \end{table}

    
\section{Análise de Vídeo}
	Para a análise de vídeo será preciso definir unha interface de liña de comandos mediante
	a cal a aplicación web chamará ao Sistema de Recoñecemento, indicándolle aqueles parámetros
	que sexan precisos\ref{fig:InterfazLineaComandos}.

	\subsection{Interface de Liña de Comandos}
	A aplicación web indicaralle a este sistema o vídeo que debe empregar como entrada para o 
	recoñecemento e o ficheiro de saía onde ten que escribir os datos da análise. A maiores, pódeselle
	indicar con que frecuencia se desexa que o Subsistema Behaviour System, encargado da análise de
	alto nivel, entre en funcionamento. Po último a opción --standar fai que se mostre o resultado da
	detección de obxectos por liña de comandos, esta saída é empregada pola capa web para calcular o
	progreso do proceso de análise.\\
	
	Estas opcións tamén se poden visualizar mediante o comando --help:\\ 
	\begin{verbatim}
		iago@UbuIago:~/TFG/src/RecognitionSystem$ ./recognitionsystem --help
		Usage:   recognitionsystem
		option:  
		-i          --input      <path/to/outputFile.xml> Set the input file.
		-o          --output     <path/to/outputFile.xml> Set the output file.
		-f [value]  --frequency  Determines after how many frames the Behaviour 
		                         System checks the minimal path
		--standar                Print the xml into the standar output.
		--verbose
	\end{verbatim}
	
	\begin{figure}[htp]
	\begin{center}
		\includegraphics[scale=0.45]{figures/InterfazLineaComandos.png}
		\caption{Interfaze de liña de comandos do Sistema de recoñecemento}
	\label{fig:InterfazLineaComandos}
	\end{center}
	\end{figure}
	
	\subsection{Ficheiro XML}
		Como resultado desta chamada, o sistema de recoñecemento debe crear no ficheiro indicado 
		para a saída, un XML co formato que se define no esquema .dtd situado no directorio
		
		\begin{verbatim}
		src/static/detections\_schema.dtd
		\end{verbatim}		 

		Neste ficheiro podemos ver a definición dos seguintes elementos:
		\begin{itemize}
		\item{{\textbf{$<objects>$\\}}}
			Contén para cada un dos fotogramas $<frame>$ a lista de obxectos detectados segundo
			o explicado no apartado \ref{cap:DeteccionObxetos}, indicando para cada un deles a
			distancia á parte esquerda, e superior da escena (xc, yc) e o alto e ancho do obxecto
			detectado (h, w).
		\item{{\textbf{$<trajectories>$\\}}}
			Este outro elemento garda para cada un dos obxectos detectados a traxectoria que 
			seguiu ao longo do vídeo, esta traxectoria estará composta de unha serie de puntos
			$<point>$, para cada un dos cales, a parte das súas coordenadas e o número de 
			frame, indicase un grao de anormalidade entre 1 e 0 que indica como de anómala é 
			a conduta dese obxecto no momento no que se atopa sobre ese punto.
		\end{itemize}
		
		Para xerar este ficheiro XML foi preciso desenvolver unha serie de funcionalidades en C++,
		que están contidas no paquete XmlRecognition.
	
	\subsection{Paquete XmlRecognition}
		Inicialmente o proxecto conta cun código escrito en C++ e apoiado en OpenCV que está distribuído en
		dúas librerías EllipseLib e BehaviorLib. A primeira delas encargada da detección de obxetos, e a segunda
		encargada de analizalo comportamento a alto nivel a partir dos resultados que proporciona a primeira.\\
		
		Para a construción do Sistema de recoñecemento integraranse estas dúas librerías como módulos, e 
		crearase un terceiro módulo C++ chamado XmlRecognition. Este novo módulo será o responsable de definir e
		implementar a interface de liña de comandos, xestionar a comunicación entre as dúas librerías e gardar
		o resultado da análise en formato XML.\\
		
		Para elo, crease un ficheiro principal chamado \textbf{main.cpp} que describe a interface de liña de comandos
		independente da librería que se emprega para as deteccións, un ficheiro \textbf{XmlUtils} que contén as
		funcionalidades para a escritura do XML en base a unha detección simplificada: DetectionDto 
		(Data Transfer Object), e por último un ficheiro \textbf{RecognitionFacade} que variará en 
		caso de cambiar as librarías, transformando o resultado destas en DetectionDto's para logo
		poder escribilo coas funcionalidades de XmlUtils.
		
		Para maximizar o rendemento evitase crear clases innecesarias: para o caso da DetectionDto é dabondo
		cunha estructura, e no caso de XmlUtils y RecognitionFacade, ao non ter estado chega con ficheiros
		que definen funcións.
		
		\begin{figure}[htp]
		\begin{center}
			\includegraphics[scale=0.25]{figures/PaqXmlRecognition.png}
			\caption{Diagrama de clases do paquete XmlRecognition}
		\label{fig:PaqXmlRecognition}
		\end{center}
		\end{figure}
		
		Coa elaboración deste paquete queda listo o sistema de recoñecemento, agora solo resta que a capa web
		sexa capaz de chamar ao sistema e mostrar as análises en XML sobre o vídeo. Será o que abordaremos
		no seguinte apartado.
		
	\subsection{A análise dende a capa web}
	
		Cando deseñamos unha aplicación web é de capital importancia que o usuario este informado de que está
		a acontecer na aplicación para que non sinta que está perdido, ou que a aplicación non responde. Tendo
		isto en conta, e sabendo que tanto o proceso de análise coma o de conversión do vídeo a outros 
		formatos poden requirir dun tempo prolongado, prantexase un problema: como manter ao usuario informado
		destes longos procesos e evitar a sensación de bloqueo?\\
		
		A solución deseñada é un sistema de notificacións que permite ao usuario rexistrado navegar libremente
		pola aplicación mentres o vídeo se está a analizar, mostrando en todo momento unha barra de progreso
		para o proceso que se está a seguir nestes intres.
		
		\begin{figure}[htp]
		\begin{center}
			\includegraphics[scale=0.5]{figures/Notificacions.png}
			\caption{Capturas de pantalla do sistema de Notificacións}
		\label{fig:Notificacions}
		\end{center}
		\end{figure}
		
		Para albergar tanto o sistema de notificacións como os procesos que engloba, creouse o módulo de 
		Django video\_upload composto polas clases que se poden observar no seguinte diagrama:

		\begin{figure}[htp]
		\begin{center}
			\includegraphics[scale=0.25]{figures/ClassDiagram.png}
			\caption{Diagrama de Clases do sistema}
		\label{fig:ClassDiagram}
		\end{center}
		\end{figure}
		
		UploadProcess representa o proceso de subida, análise, conversión e extracción de imaxes
		a partir de un vídeo. Mentres que AnalysisProcess representa o proceso que se segue no 
		caso de que un vídeo xa subido á plataforma sexa analizado de novo. Os distintos estados
		nos que pode estar un proceso modelanse mediante a clase abstracta ProcessState, que nas
		súas implementacións define tanto o traballo a realizar neste estado coma a mensaxe que 
		se amosará ao usuario cando este se execute.\\
		
		Dado que é o ProcessState que executará a tarefa, tamén será o encargado de actualizar
		a través do método set\_progress(self, progress) o progreso do proceso asociado (
		UploadProcess ou AnalysisProcess).\\
		
		É importante destacar que as figuras etiquetadas co estereotipo $<<Model>>$ son modelos 
		de BD manexados por Django. Nótese tamén que pese a que UploadProcess e AnalysisProcess
		comparten a meirande parte do seu código, non foron refactorizados nunha clase abstracta,
		dadas as complicacións de base de datos que isto carrexa. En lugar diso empregase o 
		tipado dinámico de Python para pasar os obxectos tanto de UploadProcess como de 
		AnalysisProcess á clase ProcessState, que ao invocar só os métodos comúns non é capaz de 
		percibir a diferenza
		entre ambas.
		
		O seu funcionamento mostrase no gráfico:
		
		\begin{figure}[htp]
		\begin{center}
			\includegraphics[scale=0.3]{figures/AnaliseVideoWeb.png}
			\caption{Diagrama de secuencia da análise do vídeo na capa web}
		\label{fig:AnaliseVideoWeb}
		\end{center}
		\end{figure}

\section{Mostrar Deteccións}
	
	Unha vez que o vídeo xa está subido e correctamente analizado, o que resta é comezar a construír
	na capa web as vistas que mostren sobre o elemento $<video>$ de HTML5 as distintas capas de 
	análise obtidas a partires do ficheiro XML, así como unha serie de paneis que permitan 
	configurar estas vistas.
	
	Todo este traballo realizase na vista DetailsView do módulo video\_manager e principalmente 
	consiste nun ficheiro HTML que contén as referencias a:
	\begin{itemize}
     \item O vídeo a mostrar
     \item O ficheiro XML que contén a análise realizada polo sistema de recoñecemento.
     \item Os ficheiros de estilo.
     \item Os distintos ficheiros de código javascript involucrados.
    \end{itemize}
    
    Para amosar a análise do ficheiro XML sobre o vídeo a estratexia será a de a de superpoñer 
    distintos elementos $<canvas>$ sobre o elemento vídeo $<video>$ como se pode ver na figura 
    \ref{fig:VideoTagHtml5}. Para axustar todas estas capas etiquetadas como class=``drawing-layer''
    sobre o vídeo crease a función adjustCanvasExtended do ficheiro video-player.js, esta función 
    chamase cando o vídeo se carga na páxina, en caso de que o tamaño do vídeo cambie ou cando se 
    entra e sae do modo pantalla completa, axustando de novo o tamaño de todas estas capas ao actual
    tamaño do vídeo.
    
    Unha vez axustados os elementos $<canvas>$ nos que se desexa amosar a información, tense que 
    extraer esta do ficheiro XML. O XML cargase mediante AJAX, cunha chamada de jQuery \$.get(...) 
    ao dirección do servidor que indica a etiqueta oculta con id xml\_detected\_objs, unha vez cargado
    este arquivo iniciase a carga inicial do sistema.
    
    Esta carga inicial consiste na creación dun obxecto VideoDetections creado a partir do XML, e
    que conterá a lista de deteccións así como unha referencia ao elemento $<video>$. Este elemento
    VideoDetections, é o suxeito de un patrón Observador no cal un suxeito ou obxecto central
    notifica ao seus observadores (Observers) os cambios no seu estado. Neste caso, os observadores
    serán os encargados de actualizar cada un dos elementos da páxina, incluídas as capas $<canvas>$,
    dos cambios no suxeito VideoDetections, estes cambios poden ser por exemplo o avance na 
    reprodución do vídeo, un cambio de preferencias no panel de control... Deste xeito unificase a 
    xestión das deteccións que só se manexa no elemento VideoDetections e faise moito mais sinxelo 
    engadir ou eliminar algunha capa de información xa que basta con engadir ou eliminar o 
    observador que a manexa.
    
    Nótese que a maiores do propio patrón observador, tamén se engaden os métodos enable e disable á
    clase DetectionsObserver, isto faise para que no caso de que algún observador non se atope 
    activo nun momento determinado non reciba as notificacións de actualización. O motivo deste
    cambio é o incremento de eficiencia que produce, moi importante dado que o proceso de actualización
    do Suxeito e todo-los seus observadores tense que executar entre 10 e 30 veces por segundo, todo
    isto pódese ver reflexado no diagrama da figura \ref{fig:CapaWebJS}.
    
    \begin{figure}[htp]
    \begin{center}
        \includegraphics[scale=0.35]{figures/CapaWebJS.png}
        \caption{Diagrama de clases do patrón observador na capa web}
    \label{fig:CapaWebJS}
    \end{center}
    \end{figure}
    
    Agora que se coñece o xeito no que as deteccións pasan de un formato xml a un modelo obxectual 
    no cal poden ser consultadas con maior eficiencia veremos como actualizar o estado dos elementos
    $<canvas>$ cada vez que se mostra un novo fotograma ou se modifica algunha opción dos paneis da
    páxina.
    
    Nun comezo, pensouse en asociar a actualización de estado ao evento timeupdate do elemento 
    $<video>$, que segundo a súa definición lanzase cando a posición do vídeo varía. Por desgraza, 
    e como reflexa a reflexión que podemos atopar no libro \cite[Capítulo 6.1]{video-con-html5}
    este evento é lento de mais para o seu emprego, polo que o que se fará será
    crear un bucle que se execute cando o vídeo se estea a reproducir. Para tentar que o bucle se 
    execute unha soa vez en cada un dos fotogramas empregase a función setTimeout que fará que o 
    código agarde unha cantidade de tempo antes de volver a executarse. Esta cantidade de tempo 
    calculase en base a execucións anteriores e ao número de fotogramas por segundo desa fonte de 
    vídeo, que o servidor inclúe no HTML mediante o atributo fps tras obtelos datos dunha chamada ao
    método VideoUtils.get\_fps que á súa vez chama ao ffmpeg.
    
    \begin{figure}[htp]
    \begin{center}
        \includegraphics[scale=0.2]{figures/GenerateFpsDjango.png}
        \caption{Xeración do atributo fps no lado servidor}
    \label{fig:GenerateFpsDjango}
    \end{center}
    \end{figure}
    
    Coñecido pois o xeito no que refresca o estado da interface de usuario e o sistema que crea un 
    modelo obxectual para a representación gráfica das deteccións, agora compre ver como é que se 
    remarcan os obxectos detectados no vídeo. Isto faise mediante o elemento $<canvas>$ con
    id=``objects-canvas'' no que o observador da clase DetectedObjectsObserver encargase de debuxar
    un recadro da cor axeitada. Para coñecer esta cor que depende das opcións do panel o observador
    consulta a cada obxecto Detection empregando o método getCurrentColor().
    
    Tamén cabe destacar aquí a función do observador TrainingMsgObserver, que é o encargado de 
    mostrar a etiqueta de Fotogramas de Adestramento e o recadro de cor amarelo nos primeiros 
    fotogramas do vídeo que corresponden aos empregados polo sistema de recoñecemento para obter o 
    fondo da imaxe e así poder distinguir os obxectos que se moven sobre el.
    
\section{Traxectorias}

    As traxectorias son outra parte fundamental do sistema, debese mostrar a ruta seguida por 
    cada un dos obxectos detectados dende o momento no que entra en escena ata a súa saída. Para 
    que as traxectorias sexan detectadas no sistema de recoñecemento hai que invocar á biblioteca 
    de análise de alto nivel que require de unha cantidade de tempo considerable. É por elo que na 
    interface de liña de comando precisase un argumento de frecuencia que indique cada cantos 
    fotogramas se realizará a análise de alto nivel(por defecto cada 5 fotogramas).
    
    As traxectorias chegan á capa cliente como parte do XML resultado do sistema de análise, para 
    mostralas empregarase unha nova capa canvas en conxunto cun novo observador 
    TrajectoriesObserver que itera sobre as deteccións actuais pintando as liñas entre cada un dos
    puntos da súa ruta, dende o comezo desta ate o punto no que se atopa actualmente o obxecto. 
    Estes puntos son parseados polo obxecto Detection nada mais recibir o documento XML, quedando
    almacenados como parte do seu campo trajectory.
    
    Xa que a traxectoria forma parte da análise de alto nivel, e esta 
    non é executada en tódolos fotogramas, unha traxectoria estará composta por puntos que indican a
    posición do obxecto detectado con certa frecuencia. Para que entre un punto de detección e o seu
    seguinte pareza que hai avance, dando así sensación de fluidez, o que se fai é calcular dado o
    seguinte punto e o momento actual, cal é o lugar no que debería estar dito obxecto se se movese
    en liña recta entre ambos puntos, debuxando a liña da traxectoria ate esa posición calculada.
    
    \begin{figure}[htp]
    \begin{center}
        \includegraphics[scale=0.4]{figures/calcTrajectory.png}
        \caption{Diagrama do cálculo da traxectoria}
    \label{fig:calcTrajectory}
    \end{center}
    \end{figure}
    
    O cálculo realizase en base ao ``ratio'' que indica a distancia de entre os dous puntos que o coche 
    leva percorrido.
    
    \begin{verbatim}
        var ratio = Math.abs((nFrame - f1) / (f1 - f2));
        var x = p1.x + ratio * (p2.x - p1.x);
        var y = p1.y + ratio * (p2.y - p1.y);    
    \end{verbatim}
    
    Os resultados destes cálculos gárdanse para ser empregados en frames posteriores en caso de que
    sexa preciso.

\section{Táboa de Deteccións}

    Co fin de poder ollar todas as deteccións e podelas ordenar por orde de aparición, tempo en 
    escena, etc, crease unha táboa de obxectos detectados que ademais indica os obxectos que están
    a aparecer agora mesmo no vídeo. A estes obxectos asígnaselle unha cor en función do seu 
    identificador unívoco, que ven xa composto por tres valores entre 0 e 255, e que se trata 
    segundo o model RGB como se mostra na figura \ref{fig:lightColor} para obter cores mais 
    brillantes e vistosas.
    
    \begin{figure}[htp]
    \begin{center}
        \includegraphics[scale=0.7]{figures/lightColor.png}
        \caption{Diagrama que mostra o aclarado de cores}
    \label{fig:lightColor}
    \end{center}
    \end{figure}

    Para o ordenamento da táboa empregouse a biblioteca javascript tablesorter que permite ordenar
    ascendente ou descendentemente a táboa facendo click na cabeceira do elemento polo que se desexa
    ordenar. Tamén é de destacar que os identificadores situados na primeira columna conteñen un 
    enlace que ao facer click leva o vídeo ao momento no que ese obxecto aparece por primeira vez en
    escena, función de tremenda utilidade á hora de recoñecelo.
    
    \begin{figure}[htp]
    \begin{center}
        \includegraphics[scale=0.4]{figures/DetectionsTable.png}
        \caption{Captura de pantalla que amosa a táboa de obxectos detectados}
    \label{fig:DetectionsTable}
    \end{center}
    \end{figure}
    
    Como é lóxico esta táboa actualizarase en cada un dos fotogramas se hai algún novo obxecto 
    detectado ou se algún de eles desaparece, marcando ou desmarcando este como obxectos en escena.
    Este comportamento implementase mediante dúas listas no obxecto VideoDetections: 
    \textbf{detRecentlyDeleted} e \textbf{detRecentlySelected}, que son lidas polo observador 
    DetectionsTableObserver, encargado de remarcar todas as Deteccións da lista detRecentlySelected
    e de desmarcar todas as da lista detRecentlyDeleted.
    
\section{Lista de Deteccións actuais}

    Probablemente a función mais necesaria e vistosa é a de amosar os obxectos detectados de forma 
    individualizada mentres estes están en pantalla. A estes efectos crease unha lista de deteccións
    actuais que amosa unha imaxe de tódolos obxectos presentes no vídeo.
    
    \begin{figure}[htp]
    \begin{center}
        \includegraphics[scale=0.4]{figures/detectedObjects.png}
        \caption{Captura de pantalla que amosa a lista de obxectos detectados}
    \label{fig:detectedObjects}
    \end{center}
    \end{figure}
   
    Esta sección de deteccións actuais ao igual que a táboa de obxectos detectados precisa ser 
    actualizada en cada fotograma, para elo empréganse as listas de detRecentlyDeleted e 
    detRecentlySelected explicadas anteriormente, só que agora é o observador 
    CurrentDetectionsObserver o encargado de engadir ou borrar unha detección desta sección.
    
    Tamén hai que destacar que xa que os observadores CurrentDetectionsObserver e 
    DetectionsTableObserver son os que mais tempo de execución consumen, a súa execución é 
    complementaria dependendo do estado do checkbox ``Show all detections''. De estar
    deshabilitado mantén o observador DetectionsTableObserver inactivo mentres que o 
    CurrentDetectionsObserver está activo, e estando habilitado fai o contrario. Isto obriga aos
    observadores a reimplementar os métodos ``enable()'' e ``disable()'', cargando ou
    descargando nestes métodos todos os valores actuais das deteccións que non actualizaron mentres
    estaban deshabilitados. Todo isto pódese observar no diagrama de deseño da capa Web (figura 
    \ref{fig:CapaWebJS}).
    
    
    As imaxes que se amosan como icona de cada un dos obxectos detectados son calculadas en tempo de
    reprodución do vídeo, isto é posible grazas aos elementos data URI\cite{data-uris} de javascript,
    que permiten almacenar unha imaxe como se fose unha URL e empregar esta URL para logo mostrar a 
    imaxe, por poñer o caso, no fondo (background-image) de un elemento HTML. Esta URI calcúlase 
    no método da clase VideoDetections ``captureImg'' empregando unha chamada a ``canvas.toDataURL()''
    onde canvas é un elemento HTMLCanvasElement obtido a partir do fotograma actual do vídeo.
    
    Por último para mostrar a información asociada a este obxecto detectado de algún xeito, 
    empregase a compoñente da librería bootstrap popover, que é unha ventá flotante que xurde ao
    carón da detección cando o rato pasa por enriba dela. Este popover personalizase coa cor 
    concreta desa detección.
    
    \begin{figure}[htp]
    \begin{center}
        \includegraphics[scale=0.5]{figures/popoverCapture.png}
        \caption{Captura de pantalla que amosa o popover da lista de deteccións actuais}
    \label{fig:popoverCapture}
    \end{center}
    \end{figure}

    
\section{Comportamento anormal}

    Un dos factores nos que destacan os algoritmos dos laboratorios VARPA é precisamente a análise
    de comportamento co fin de detectar o comportamento anormal de un dos obxectos implicados nunha
    escena. Isto faise calculando cal é o camiño polo que un obxecto soe viaxar do punto A ao 
    punto B, para calquera que sexan os puntos A e B, de xeito que podemos medir o anormal que é
    o seu comportamento en función do que o seu camiño se diferencie do camiño habitual. Isto pode
    axudar a detectar por exemplo un peón que non cruza polo paso de cebra, un coche facendo unha 
    manobra perigosa...
    
    O sistema de recoñecemento facilitado polo laboratorio foi deseñado para poder devolver un ratio
    de esa anormalidade en cada unha das chamadas ao sistema de análise de Alto nivel, que como se 
    dixo anteriormente executase cada determinado período de tempo. E dado que é este sistema o que
    proporciona a traxectoria do obxecto, o ratio de anormalidade para cada un dos momentos 
    asociouse pois a cada un dos puntos da traxectoria como se pode ver na imaxe \ref{fig:abXmlRate}.
    Como se pode comprobar nesta imaxe, existen unha serie de momentos iniciais nos que o ratio inda
    non puido ser calculado, e neste caso o valor asociado será 0.
    
    \begin{figure}[htp]
    \begin{center}
        \includegraphics[scale=0.5]{figures/abXmlRate.png}
        \caption{Ratio de anormalidade gardado nun documento XML}
    \label{fig:abXmlRate}
    \end{center}
    \end{figure}
    
    Non obstante un número é algo moi pouco gráfico, que dificilmente pode axudar a resaltar unha 
    detección anómala de unha que non o é, por este motivo se deseñou unha serie de compoñentes que
    permiten seleccionar un límite (un ratio de anormalidade máximo) a partir do cal un obxecto será
    considerado sospeitoso. A nivel de interface web, este límite pode marcarse cunha barra 
    selectora chamada slider, que  se importou da libraría jQuery UI\cite{ComponenteSliderJqueryUi},
    e que se complementou cunha entrada de texto para dar a posibilidade de que o valor se seleccione
    ou ben escribíndoo, ou ben a través do slider. Os valores a seleccionar variarán sempre entre 0
    e o máximo valor atopado entre os ratios de anormalidade das deteccións.

    \begin{figure}[htp]
    \begin{center}
        \includegraphics[scale=0.5]{figures/sliderCapture.png}
        \caption{Captura de pantalla do compoñente slider da biblioteca jQuery UI}
    \label{fig:sliderCapture}
    \end{center}
    \end{figure}
    
    Agora que xa se pode seleccionar o valor a partir do cal queremos marcar unha detección como
    anómala, falemos do xeito no que se vaia realizar este marcado. A idea xeral consiste en 
    inabilitar o uso de diferentes cores e marcar de azul todas as deteccións, con excepción de 
    aquelas que teñan un comportamento anormal que serán marcadas de vermello, e aquelas cuxo 
    comportamento inda non fose analizado que serán marcada en cor negra. Este uso de cores no só
    se emprega nas capas $<canvas>$ de información que se amosan sobre o vídeo senón tamén na táboa
    de obxectos detectados e na lista de deteccións actuais.

    \begin{figure}[htp]
    \begin{center}
        \includegraphics[scale=0.4]{figures/suspiciousDetections.png}
        \caption{Captura de pantalla que amosa o comportamento das deteccións sospeitosas}
    \label{fig:suspiciousDetections}
    \end{center}
    \end{figure}
    
    Para conseguir isto, centralizase a xestión da cor sobre o obxecto Detection, facendo que sexa 
    este o que determine de forma unívoca de que cor se ten que debuxar, para isto creanse os 
    métodos ``getCurrentColor'' que devolve a cor para as capas $<canvas>$ e 
    ``getCurrentLightColor'' que devolve un color mais claro para a táboa e a lista de deteccións.
    O obxecto Detection á súa vez calculara a cor a devolver en función do seu ratio de anormalidade
    e o valor das propiedade: videoDetections.alarmAbnormalRate, videoDetections.useAbnormalityRate
    e videoDetections.useColors.
    
    A parte deste mecanismo para centralizar a xestión do color que será empregado por tódolos 
    observadores, os observadores DetectionsTableObserver e CurrentDetectionsObserver precisan 
    coñecer aquelas deteccións que cambiaron a un estado sospeitoso, de confianza ou sen datos,
    con este obxectivo crease a lista VideoDetections.abChangingDetections, que indicará aquelas
    deteccións que cambiaron de estado e polo tanto, no caso da táboa e a lista de deteccións, 
    precisan mudar a súa cor.
    

\section{Popups de deteccións sospeitosas}

    Para completar a funcionalidade de detectar o comportamento anómalo, solicitase unha faceta a 
    maiores do marcado das deteccións sospeitosas, deberase dar a opción de lanzar unha nova ventá
    por cada obxecto sospeitoso detectado coa posibilidade de ver este obxecto de preto ao largo do
    seu percorrido.
    
    Tras estudar varios xeito de facer isto, a mellor solución parece ser a de empregar pop-ups 
    tamén chamados ventás emerxentes, que inda que normalmente se evitan por estares asociados 
    con contido publicitario, neste caso compren á perfección coa tarefa que se desexa realizar.
    
    Xa que haberá que ampliar a imaxe facendo un efecto zoom para poder mostrar a zoa do vídeo na
    que está a detección con mais detalle, prantexase a construción dun sistema que mediante unha
    barra selectora (slider) permita achegarse ou afastarse para ver mais de preto ou de lonxe o 
    obxecto detectado. Para estas ventás emerxentes empregarase un novo ficheiro .css e tamén un 
    novo ficheiro .js, que conterá os mecanismos precisos para crear este efecto zoom.
    
    A esta nova ventá pasaranse como parámetros da URL o identificador da detección e a ULR do 
    ficheiro XML que contén a análise. Con estes datos o ficheiro javascript principal chamado
    suspicious-popup.js, que segue unha distribución similar ao de video-player.js para a páxina
    que amosa o vídeo, obterá o ficheiro XML e en base a el iniciará o sistema. Esta
    páxina tamén constará dun elemento $<video>$, con capas $<canvas>$ superpostas, a primeira delas 
    ocultará o vídeo por completo mostrando a porción del que sexa precisa para lograr o efecto 
    zoom e as sucesivas mostrarán a información desexada.
    
    Ao manter a estrutura dun elemento $<video>$ coas capas $<canvas>$ superpostas, a mellor 
    arquitectura posible e a dun patrón Observador, no que o suxeito manteña ademais das referencias
    ao vídeo, os datos sobre o nivel de zoom, que será necesariamente empregado polos seus 
    observadores. Para esta tarefa reempregouse gran parte do deseño do diagrama \ref{fig:CapaWebJS}
    engadindo as características precisas nunha clase herdada de VideoDetections que se nomea 
    ZoomVideoDetections como se pode ver na figura \ref{fig:SuspiciousPopupJS}.
    
    \begin{figure}[htp]
    \begin{center}
        \includegraphics[scale=0.4]{figures/SuspiciousPopupJS.png}
        \caption{Diagrama de clases para a ventá emerxente de deteccións sospeitosas}
    \label{fig:SuspiciousPopupJS}
    \end{center}
    \end{figure} 
    
    Este sistema é iniciado na carga do XML e ao igual que na páxina que reproduce o vídeo, cando a 
    carga finaliza crease
    un bucle coa función updateStatus que actualizará en cada fotograma o estado dos compoñentes 
    chamando á función ZoomVideoDetections.updateState(). Esta función tamén é chamada cando o vídeo
    lanza o evento play, ou ao cambiar o nivel de zoom mediante a barra selectora (slider).

    
    \begin{figure}[htp]
    \begin{center}
        \includegraphics[scale=0.4]{figures/suspiciousPopupCapture.png}
        \caption{Captura de pantalla que amosa a ventá de deteccións sospeitosas}
    \label{fig:suspiciousPopupCapture}
    \end{center}
    \end{figure} 
    
    A continuación explicase en detalle o mecanismo empregado para crear o efecto zoom e para 
    levar a cabo o seguimento do obxecto mantendo este nivel de zoom. Empréganse 
    basicamente dúas variables, por un lado o tamaño da fracción de vídeo a amosar (w\_size), que
    variará entre un mínimo(dúas veces o tamaño da detección) e un máximo que é o tamaño do vídeo, 
    e por outro lado o centro (center) da escena que será unha variable composta de dúas 
    compoñentes (x,y) que indican a distancia en píxeles á parte superior esquerda do vídeo.
    
    A función ZoomVideoDetections.centerToLeftTop é a encargada de calcular dado un tamaño de ventá 
    e un centro, esta distancia á marxe esquerda superior do vídeo. Este cálculo no é sinxelo, xa que 
    hai que evitar que o recadro a seleccionar exceda das dimensións do vídeo. Esta casuística 
    amosase na figura \ref{fig:centerToLeftTop}
    
    \begin{figure}[htp]
    \begin{center}
        \includegraphics[scale=0.8]{figures/centerToLeftTop.png}
        \caption{Diagrama que explica os problemas resoltos para implementar o efecto zoom}
    \label{fig:centerToLeftTop}
    \end{center}
    \end{figure}
    
    
    
    
    
    
    
    
    
    
