\chapter{Desenvolvemento}
\label{cap:desenvolvemento}

\section{Funcionalidades desexadas}
    Antes de comezar o ciclo de sprint's tivo lugar unha pequena fase de análise no que se 
    requiriron as seguintes funcionalidades do proxecto a realizar:
    \begin{itemize}
     \item \textbf{Control de Usuarios:} É preciso un mínimo control de usuarios para controlar que
        sube vídeos á plataforma.
     \item \textbf{Carga de Vídeo:} Dado que a aplicación traballará con vídeos subidos polos 
        usuarios, o primeiro paso é lograr a subida exitosa de vídeos á plataforma por parte de
        usuarios autenticados.
     \item \textbf{Reprodución de Vídeo:} Desexase que a aplicación permita a reprodución dos vídeos
        contidos, mediante técnicas de streaming ou pseudo-streaming.
     \item \textbf{Lista de vídeos e Imaxe de Portada:} É preciso amosar de algún modo os vídeos 
        subidos á plataforma, a poder ser cunha imaxe representativa que axude a identificalos e con
        algún mecanismo de busca que permita filtralos en caso de que haxa un número elevado de eles.
     \item \textbf{Análise de Vídeo:} A aplicación web debe facer uso de dúas librerías de 
        computación visual proporcionadas polo laboratorio VARPA para analizar os vídeos subidos e
        extraer de eles os obxectos detectados e unha análise do seu comportamento, como é lóxico só
        os usuarios autenticados porán facer isto xa que só eles poden subir vídeos á aplicación.
     \item \textbf{Mostrar Deteccións:} Devénse amosar aqueles obxectos detectados mediante unha 
        capa de información extra sobre o vídeo subido. 
     \item \textbf{Traxectorias:} Tamén se han de indicar con unha capa de información sobre o vídeo
        as traxectorias seguidas polos obxectos detectados.
     \item \textbf{Listar Deteccións:} É importante listar os obxectos detectados en todo o vídeo coa
        súa información asociada, así como listar tamén aqueles que están en escena nun momento 
        determinado.
     \item \textbf{Detección do comportamento anómalo:} Desexase que empregando as funcionalidades
        da biblioteca proporcionada se analice o comportamento dos obxectos involucrados nunha 
        escena para así poder filtralos segundo o estraño que resulte este comportamento.
     \item \textbf{Popups de deteccións sospeitosas:} Outra funcionalidade requirida é que cando un
        obxecto teña un comportamento sospeitoso respecto ao do resto de obxectos, se abra unha nova
        ventá que o siga para poder ver mais de preto o que está a facer.
    \end{itemize}

O desenvolvemento da aplicación tivo lugar seguindo a filosofía de SCRUM, polo que o enfoque mais 
correcto neste caso para ver como se cumpriron todas as funcionalidades anteriores é o de explicar 
o traballo acometido en cada un dos sprints.
  
\section{v0.1 Cargar e Visualizar vídeos}
    Neste primeiro Sprint a parte de adicar moitas horas á formación no framework de Django e no 
    traballo con GitHub, creouse o esquema básico sobre o que se partirá para a creación e toda a
    web. Este esquema está baseado no plantilla Edge\cite{edge-templ}, que proporciona un control 
    mínimo de usuarios, unha liña de estilo, integración con bootstrap e control de administrador,
    inda que posteriormente a maioría destes aspectos tiveron que ser modificados ou ampliados.
    
    A partir desta plantilla que xa proporciona o control de usuarios, as dúas primeiras 
    funcionalidades a implementar foron a Carga dos vídeos e a súa reprodución.
    
    \subsection{Carga de Vídeo}
        Dado que a aplicación traballará con vídeos subidos polos usuarios, o primeiro paso é lograr
        a subida exitosa de vídeos á plataforma. Para este fin empregarase un formulario HTML que 
        viaxa sobre unha chamada POST de HTTP (figura \ref{fig:SubidaVideoForm}). 
        Cando o navegador faga esta chamada incluíndo o vídeo como parte do formulario, este vídeo
        comezará a subirse ao servidor en pequenos anaquiños (data chunk).
        
        \begin{figure}[htp]
        \begin{center}
            \includegraphics[scale=0.6]{figures/SubidaVideoForm.png}
            \caption{Formulario para a creación de vídeos}
        \label{fig:SubidaVideoForm}
        \end{center}
        \end{figure}    
        
        É de especial importancia que no caso de que o vídeo teña un peso considerable e precise 
        duns cantos segundos para subirse á plataforma, o usuario poida coñecer de forma gráfica
        o avance deste proceso.
        
        Con este fin, crease un sistema de notificación de progreso baseado no 
        django-progressbarupload \cite{django-progressbarupload}, este sistema apoiase nunha compoñente
        fundamental chamada VideoUploadHandler, que é unha extensión da interface de Django 
        TemporaryFileUploadHandler \cite{TemporaryFileUploadHandler}, e que basicamente manexa a subida
        dun ficheiro de tamaño considerable.\\
        
        Esta compoñente componse dunha función de inicio (handle\_raw\_input) que crea unha entrada 
        na Cache de Django, almacenando como chave un número aleatorio e a IP do cliente que está a
        subir o vídeo, e como valor o tamaño do ficheiro e o porcentaxe de este que xa está subido.
        Esta entrada será actualizada cada vez que o servidor reciba un novo anaco de vídeo (mediante
        a outra función do compoñente VideoUploadHandler, receive\_data\_chunk). 
        
        Por outra parte, para que visualmente o cliente poida ver o avance da subida a través unha
        barra de progreso, crease unha función asíncrona en javascript (Tecnoloxía AJAX), que 
        periodicamente consulta ao servidor para obter o valor da cache que indica a porcentaxe de
        subida do vídeo, e unha vez obtido, actualiza a barra de progreso para mostralo. Todo isto
        ten lugar no navegador mentres este inda está a subir o arquivo de vídeo.

        Unha vez que a subida se completa, o POST é manexado pola vista UploadView, que se todos
        os datos do formulario son correctos, encargase de crear un modelo VideoModel. Como parte
        desta creación o vídeo pasa do directorio temporal no que foi almacenado (baixo linux por 
        defecto é /tmp) a un directorio calculado pola función get\_valid\_filename. Esta función
        pásaselle ao modelo como parte do seu campo ''video'' do tipo FileField. Todo este proceso
        pódese ver de xeito mais claro no diagrama \ref{fig:SubidaVideo1} e \ref{fig:SubidaVideo2}.
        
        \begin{figure}[htp]
        \begin{center}
            \includegraphics[scale=0.45]{figures/SubidaVideo.pdf}
            \caption{Diagrama de secuencia do proceso seguido cando se fai o Submit do Formulario 
            de creación de vídeos (1)}
        \label{fig:SubidaVideo1}
        \end{center}
        \end{figure}
        
        \begin{figure}[htp]
        \begin{center}
            \includegraphics[scale=0.45]{figures/FinSubidaVideo.pdf}
            \caption{Diagrama de secuencia do proceso seguido cando se fai o Submit do Formulario 
            de creación de vídeos (2)}
        \label{fig:FinSubidaVideo2}
        \end{center}
        \end{figure}
                
        
        
        Unha vez subido o vídeo satisfactoriamente agora debese reproducir ese vídeo subido nunha 
        páxina adicada.
            
    \subsection{Reprodución de Vídeo}
        Desexase que a aplicación permita a reprodución dos vídeos contidos, mediante técnicas de
        streaming ou pseudo-streaming. Neste caso empregarase o pseudo-streaming polo sinxela que
        resulta esta implementación empregando as capacidades da etiqueta $<video>$ de HTML5 en conxunto
        con un servidor HTTP como Apache ou o servidor para desenvolvemento de Django.

        \begin{figure}[htp]
        \begin{center}
            \includegraphics[scale=0.55]{figures/VideoTagHtml5.png}
            \caption{tag en html5, coas súas fontes e coas capas $<canvas>$ asociadas}
        \label{fig:VideoTagHtml5}
        \end{center}
        \end{figure}
        
        na figura \ref{fig:VideoTagHtml5} podemos ver o resultado en HTML5, vense claramente a etiqueta
        $<video>$ coas súas fontes de datos $<source>$, cabe destacar que aquí engadiuse o atributo fps
        (Fotogramas Por Segundo do inglés Frames per second) que non pertence ao estándar definido pola 
        W3C\ref{w3schools-source-tag} mais no caso da nosa aplicación é fundamental para poder coñecer 
        a velocidade á que o navegador vai amosar os fotogramas do vídeo.
        
        Outra cuestión a aclarar é o motivo polo cal non se subministra a fonte de vídeo en formato
        .ogv que a W3C recomenda. A resposta é que o codec theora que ffmpeg inclue soporta 
        decodificación pero NON codificación, polo cal é posible pasar de vídeos en .ogv a outros 
        formatos pero non de outros formatos a .ogv imposibilitando pois que se poida ofrecer o 
        vídeo neste formato mentres o codec de ffmpeg non o permita. Non obstante, isto non supón un 
        problema, xa que como se pode ver na táboa seguinte todos os navegadores permiten a reprodución
        baseándose nestes dous formatos:
        
        \begin{table}[]
            \centering
            \label{supported-video-formats}
            \begin{tabular}{llll}
            \hline
                \multicolumn{1}{c}{\bfseries Navegador} & 
                \multicolumn{1}{c}{\bfseries MP4} & 
                \multicolumn{1}{c}{\bfseries WebM} & 
                \multicolumn{1}{c}{\bfseries Ogg} \\

                \rowcolor[HTML]{EFEFEF} 
                Internet Explorer & SI                                                                                              & NON & NON \\
                Chrome            & SI                                                                                              & SI  & SI  \\
                \rowcolor[HTML]{EFEFEF} 
                Firefox           & \begin{tabular}[c]{@{}l@{}}SI \\ dende Firefox 21 (win)\\ dende Firefox 30 (linux)\end{tabular} & SI  & SI  \\
                Safari            & SI                                                                                              & NON & NON \\
                \rowcolor[HTML]{EFEFEF} 
                Opera             & \begin{tabular}[c]{@{}l@{}}SI\\ dende Opera 25\end{tabular}                                     & SI  & SI 
            \end{tabular}
            \caption{Táboa de formatos de vídeo soportados polos distintos navegadores}
        \end{table}

    
\section{v0.2  Subida e conversión de vídeos }

    Nesta segunda iteración, deséxanse acometer dúas tarefas principais que teñen que ver coa 
    usabilidade da aplicación. Por unha parte crear un listado de vídeos subidos á aplicación para 
    poder acceder a eles de xeito ordenado, e por
    outra parte interesa dar soporte aos procesos de conversión do vídeos a distintos formatos, 
    á análise do vídeo(que se implementará en futuras iteracións) e á extracción de imaxes para 
    empregalas como imaxes representativas do vídeo, dunha forma fluída e estética.
    
    \subsection{Lista de vídeos e Imaxe de Portada}
        
        Co fin de que os usuarios accedan aos distintos vídeos subidos, deseñase unha páxina web que será a
        principal do módulo video\_manager na cal un usuario pode visualizar unha \textbf{ lista paxinada dos
        vídeos} dispoñibles. Esta lista estará ordenada comezando polo vídeo mais recente e rematando
        polo mais antigo, tamén se contempla a posibilidade de poder filtralos por exemplo polo nome.
        
        Para a elaboración de esta páxina empreganse o elementos dos que dispón Django como son a
        clase ListView co seu atributo paginate\_by e o filtrado de resultado co método 
        QuerySet.filter(...), mentres que para o paso das palabras claves polas que buscar un vídeo
        empregase un parámetro de URL chamado 'name'.\\
        
        Outra funcionalidade interesante de cara a amosar unha lista de vídeos é a de poder mostrar unha
        imaxe representativa de cada un deles. Con este fin crease a vista SuccessfulUpload (imaxe
        \ref{fig:SuccessfulUploadScreen}), á que se redirecciona unha vez o vídeo é subido correctamente
        para seleccionar a súa \textbf{imaxe de portada}.
        
        \begin{figure}[htp]
        \begin{center}
            \includegraphics[scale=0.35]{figures/SuccessfulUploadScreen.png}
            \caption{Captura de pantalla da páxina web SuccessfulUpload}
        \label{fig:SuccessfulUploadScreen}
        \end{center}
        \end{figure}
        
        Con este fin extráense mediante ffmpeg unha serie de fotogramas do vídeo, que se gardan nun
        directorio temporal para que unha vez o vídeo estea subido e analizado, as imaxes se integren 
        como parte dun formulario na páxina SuccessfulUpload. Mediante este formulario, xerado co plugin
        image-picker\cite{ImagePickerPage}, o usuario poderá escoller o fotograma que lle pareza mais
        representativo do vídeo e unha vez que pulse no botón de ''Submit'' este fotograma gardarase
        como parte do Modelo de Django VideoModel, quedando pois accesible para que a lista de vídeos 
        poida amosalo.
        
    \subsection{Sistema de Notificacións}
    
        Cando deseñamos unha aplicación web é de capital importancia que o usuario este informado de que está
        a acontecer na aplicación para que non sinta que está perdido, ou que a aplicación non responde. Tendo
        isto en conta, e sabendo que tanto o proceso de análise(que inda non está implementado)
        coma o de conversión do vídeo a outros 
        formatos poden requirir dun tempo prolongado, prantexase un problema: como manter ao usuario informado
        destes longos procesos e evitar a sensación de bloqueo?\\
        
        A solución deseñada é un sistema de notificacións que permite ao usuario rexistrado navegar libremente
        pola aplicación mentres o vídeo se está a analizar, mostrando en todo momento unha barra de progreso
        para o proceso que se está a seguir nestes intres como se pode observar na imaxe 
        \ref{fig:Notificacions}.
        
        \begin{figure}[htp]
        \begin{center}
            \includegraphics[scale=0.5]{figures/Notificacions.png}
            \caption{Capturas de pantalla do sistema de Notificacións}
        \label{fig:Notificacions}
        \end{center}
        \end{figure}
        
        Para albergar tanto o sistema de notificacións como os procesos que engloba, creouse o módulo de 
        Django video\_upload composto polas clases que se poden observar no diagrama \ref{fig:ClassDiagram}.

        \begin{figure}[htp]
        \begin{center}
            \includegraphics[scale=0.4]{figures/ClassDiagram.pdf}
            \caption{Diagrama de Clases do sistema de subida}
        \label{fig:ClassDiagram}
        \end{center}
        \end{figure}
        
        UploadProcess representa o proceso de subida, análise, conversión e extracción de imaxes
        a partir de un vídeo. Mentres que AnalysisProcess representa o proceso que se segue no 
        caso de que un vídeo xa subido á plataforma sexa analizado de novo. Os distintos estados
        nos que pode estar un proceso modelanse mediante a clase abstracta ProcessState, que nas
        súas implementacións define tanto o traballo a realizar neste estado coma a mensaxe que 
        se amosará ao usuario cando este se execute.\\
        
        Dado que é o ProcessState que executará a tarefa, tamén será o encargado de actualizar
        a través do método set\_progress(self, progress) o progreso do proceso asociado (
        UploadProcess ou AnalysisProcess).\\
        
        É importante destacar que as figuras etiquetadas co estereotipo $<<Model>>$ son modelos 
        de BD manexados por Django. Nótese tamén que pese a que UploadProcess e AnalysisProcess
        comparten a meirande parte do seu código, non foron refactorizados nunha clase abstracta,
        dadas as complicacións de base de datos que isto carrexa. En lugar diso empregase o 
        tipado dinámico de Python para pasar os obxectos tanto de UploadProcess como de 
        AnalysisProcess á clase ProcessState, que ao invocar só os métodos comúns non é capaz de 
        percibir a diferenza entre ambas. O seu funcionamento mostrase no gráfico
        \ref{fig:AnaliseVideoWeb}.
        
        \begin{figure}[htp]
        \begin{center}
            \includegraphics[scale=0.35]{figures/AnaliseVideoWeb.pdf}
            \caption{Diagrama de secuencia da análise do vídeo na capa web}
        \label{fig:AnaliseVideoWeb}
        \end{center}
        \end{figure}
    
\section{v0.3 Xerar e mostrar deteccións básicas}

    En esta iteración tratouse tanto a análise do vídeo creando o sistema de recoñecemento, como 
    o xeito de amosar os resultados destes na capa web, todo elo deseñado de xeito flexible e 
    modulado como se explica a continuación.
    
    \subsection{Análise de Vídeo}
        Para a análise de vídeo será preciso definir unha interface de liña de comandos mediante
        a cal a aplicación web chamará ao Sistema de Recoñecemento, indicándolle aqueles parámetros
        que sexan precisos\ref{fig:InterfazLineaComandos}.

        \subsubsection{Interface de Liña de Comandos}
        A aplicación web indicaralle a este sistema o vídeo que debe empregar como entrada para o 
        recoñecemento e o ficheiro de saía onde ten que escribir os datos da análise. A maiores, pódeselle
        indicar con que frecuencia se desexa que o Subsistema Behaviour System, encargado da análise de
        alto nivel, entre en funcionamento. Po último a opción --standar fai que se mostre o resultado da
        detección de obxectos por liña de comandos, esta saída é empregada pola capa web para calcular o
        progreso do proceso de análise.\\
        
        Estas opcións tamén se poden visualizar mediante o comando --help:\\ 
        \begin{verbatim}
    iago@UbuIago:~/TFG/src/RecognitionSystem$ ./recognitionsystem --help
    Usage:   recognitionsystem
    option:  
    -i          --input      <path/to/outputFile.xml> Set the input file.
    -o          --output     <path/to/outputFile.xml> Set the output file.
    -f [value]  --frequency  Determines after how many frames the Behaviour 
                            System checks the minimal path
    --standar                Print the xml into the standar output.
    --verbose
        \end{verbatim}
        
        \begin{figure}[htp]
        \begin{center}
            \includegraphics[scale=0.45]{figures/InterfazLineaComandos.png}
            \caption{Interfaze de liña de comandos do Sistema de recoñecemento}
        \label{fig:InterfazLineaComandos}
        \end{center}
        \end{figure}
        
        \subsubsection{Ficheiro XML}
            Como resultado desta chamada, o sistema de recoñecemento debe crear no ficheiro indicado 
            para a saída, un XML co formato que se define no esquema .dtd situado no directorio
            
            \begin{verbatim}
            src/static/detections\_schema.dtd
            \end{verbatim}       

            Neste ficheiro podemos ver a definición dos seguintes elementos:
            \begin{itemize}
            \item{{\textbf{$<objects>$\\}}}
                Contén para cada un dos fotogramas $<frame>$ a lista de obxectos detectados segundo
                o explicado no apartado \ref{cap:DeteccionObxetos}, indicando para cada un deles a
                distancia á parte esquerda, e superior da escena (xc, yc) e o alto e ancho do obxecto
                detectado (h, w).
            \item{{\textbf{$<trajectories>$\\}}}
                Este outro elemento garda para cada un dos obxectos detectados a traxectoria que 
                seguiu ao longo do vídeo, esta traxectoria estará composta de unha serie de puntos
                $<point>$, para cada un dos cales, a parte das súas coordenadas e o número de 
                frame, indicase un grao de anormalidade entre 1 e 0 que indica como de anómala é 
                a conduta dese obxecto no momento no que se atopa sobre ese punto.
            \end{itemize}
            
            Para xerar este ficheiro XML foi preciso desenvolver unha serie de funcionalidades en C++,
            que están contidas no paquete XmlRecognition.
        
        \subsubsection{Paquete XmlRecognition}
            Inicialmente o proxecto conta cun código escrito en C++ e apoiado en OpenCV que está distribuído en
            dúas librerías EllipseLib e BehaviorLib. A primeira delas encargada da detección de obxetos, e a segunda
            encargada de analizalo comportamento a alto nivel a partir dos resultados que proporciona a primeira.\\
            
            Para a construción do Sistema de recoñecemento integraranse estas dúas librerías como módulos, e 
            crearase un terceiro módulo C++ chamado XmlRecognition (figura \ref{fig:PaqXmlRecognition}).
            Este novo módulo será o responsable de definir e implementar a interface de liña de 
            comandos, xestionar a comunicación entre as dúas librerías e gardar o resultado da análise
            en formato XML.
            
            \begin{figure}[htp]
            \begin{center}
                \includegraphics[scale=0.4]{figures/PaqXmlRecognition.pdf}
                \caption{Diagrama de clases do paquete XmlRecognition}
            \label{fig:PaqXmlRecognition}
            \end{center}
            \end{figure}
            
            Para elo, crease un ficheiro principal chamado \textbf{main.cpp} que describe a interface de liña de comandos
            independente da librería que se emprega para as deteccións, un ficheiro \textbf{XmlUtils} que contén as
            funcionalidades para a escritura do XML en base a unha detección simplificada: DetectionDto 
            (Data Transfer Object), e por último un ficheiro \textbf{RecognitionFacade} que variará en 
            caso de cambiar as librarías, transformando o resultado destas en DetectionDto's para logo
            poder escribilo coas funcionalidades de XmlUtils.
            
            Para maximizar o rendemento evitase crear clases innecesarias: para o caso da DetectionDto é dabondo
            cunha estrutura, e no caso de XmlUtils y RecognitionFacade, ao non ter estado chega con ficheiros
            que definen funcións.
            
            Coa elaboración deste paquete queda listo o sistema de recoñecemento, agora solo resta que a capa web
            sexa capaz de chamar ao sistema e mostrar as análises en XML sobre o vídeo. Será o que abordaremos
            no seguinte apartado.

    \subsection{Mostrar Deteccións}
        
        Unha vez que o vídeo xa está subido e correctamente analizado, o que resta é comezar a construír
        na capa web as vistas que mostren sobre o elemento $<video>$ de HTML5 as distintas capas de 
        análise obtidas a partires do ficheiro XML, así como unha serie de paneis que permitan 
        configurar estas vistas.
        
        Todo este traballo realizase na vista DetailsView do módulo video\_manager e principalmente 
        consiste nun ficheiro HTML que contén as referencias a:
        \begin{itemize}
        \item O vídeo a mostrar
        \item O ficheiro XML que contén a análise realizada polo sistema de recoñecemento.
        \item Os ficheiros de estilo.
        \item Os distintos ficheiros de código javascript involucrados.
        \end{itemize}
        
        Para amosar a análise do ficheiro XML sobre o vídeo a estratexia será a de a de superpoñer 
        distintos elementos $<canvas>$ sobre o elemento vídeo $<video>$ como se pode ver na figura 
        \ref{fig:VideoTagHtml5}. Para axustar todas estas capas etiquetadas como class=``drawing-layer''
        sobre o vídeo crease a función adjustCanvasExtended do ficheiro video-player.js, esta función 
        chamase cando o vídeo se carga na páxina, en caso de que o tamaño do vídeo cambie ou cando se 
        entra e sae do modo pantalla completa, axustando de novo o tamaño de todas estas capas ao actual
        tamaño do vídeo.
        
        Unha vez axustados os elementos $<canvas>$ nos que se desexa amosar a información, tense que 
        extraer esta do ficheiro XML. O XML cargase mediante AJAX, cunha chamada de jQuery \$.get(...) 
        ao dirección do servidor que indica a etiqueta oculta con id xml\_detected\_objs, unha vez cargado
        este arquivo iniciase a carga inicial do sistema.
        
        Esta carga inicial consiste na creación dun obxecto VideoDetections creado a partir do XML, e
        que conterá a lista de deteccións así como unha referencia ao elemento $<video>$. Este elemento
        VideoDetections, é o suxeito de un patrón Observador no cal un suxeito ou obxecto central
        notifica ao seus observadores (Observers) os cambios no seu estado. Neste caso, os observadores
        serán os encargados de actualizar cada un dos elementos da páxina, incluídas as capas $<canvas>$,
        dos cambios no suxeito VideoDetections, estes cambios poden ser por exemplo o avance na 
        reprodución do vídeo, un cambio de preferencias no panel de control... Deste xeito unificase a 
        xestión das deteccións que só se manexa no elemento VideoDetections e faise moito mais sinxelo 
        engadir ou eliminar algunha capa de información xa que basta con engadir ou eliminar o 
        observador que a manexa.
        
        Nótese que a maiores do propio patrón observador, tamén se engaden os métodos enable e disable á
        clase DetectionsObserver, isto faise para que no caso de que algún observador non se atope 
        activo nun momento determinado non reciba as notificacións de actualización. O motivo deste
        cambio é o incremento de eficiencia que produce, moi importante dado que o proceso de actualización
        do Suxeito e todo-los seus observadores tense que executar entre 10 e 30 veces por segundo, todo
        isto pódese ver reflexado no diagrama da figura \ref{fig:observerPattern}.
        
        \begin{figure}[htp]
        \begin{center}
            \includegraphics[scale=0.35]{figures/observerPattern.pdf}
            \caption{Diagrama de clases simplificado do patrón observador na capa web}
        \label{fig:observerPattern}
        \end{center}
        \end{figure}
        
        Agora que se coñece o xeito no que as deteccións pasan de un formato xml a un modelo obxectual 
        no cal poden ser consultadas con maior eficiencia veremos como actualizar o estado dos elementos
        $<canvas>$ cada vez que se mostra un novo fotograma ou se modifica algunha opción dos paneis da
        páxina.
        
        Nun comezo, pensouse en asociar a actualización de estado ao evento timeupdate do elemento 
        $<video>$, que segundo a súa definición lanzase cando a posición do vídeo varía. Por desgraza, 
        e como reflexa a reflexión que podemos atopar no libro \cite[Capítulo 6.1]{video-con-html5}
        este evento é lento de mais para o seu emprego, polo que o que se fará será
        crear un bucle que se execute cando o vídeo se estea a reproducir. Para tentar que o bucle se 
        execute unha soa vez en cada un dos fotogramas empregase a función setTimeout que fará que o 
        código agarde unha cantidade de tempo antes de volver a executarse. Esta cantidade de tempo 
        calculase en base ás dez execucións anteriores e ao número de fotogramas por segundo como se
        pode ver na seguinte formula:
        
        \vspace{1cm}
        \begingroup\makeatletter\def\f@size{16}\check@mathfonts
            $ t_n = \frac{1000}{videoDetections.fps} - \frac{\sum_{i=0}^{10} lastUpdateTimes[i]}{10}$
        \endgroup
        \vspace{1cm}
        
        O número de fotogramas por segundo para esa fonte ao que se accede mediante o obxecto 
        videoDetections obtense grazas a que o servidor o inclúe no HTML mediante o atributo fps en 
        cada elemento $<source>$ do elemento $<video>$ como se ve na figura \ref{fig:VideoTagHtml5}.
        Á súa vez o servidor obtén estes datos para as distintas fontes de vídeo dunha chamada ao
        método VideoUtils.get\_fps que chama ao ffmpeg como se ve no diagrama de secuencia 
        \ref{fig:GenerateFpsDjango}.
        
        \begin{figure}[htp]
        \begin{center}
            \includegraphics[scale=0.5]{figures/GenerateFpsDjango.pdf}
            \caption{Xeración do atributo fps no lado servidor}
        \label{fig:GenerateFpsDjango}
        \end{center}
        \end{figure}
                
        Coñecido pois o xeito no que refresca o estado da interface de usuario e o sistema que crea un 
        modelo obxectual para a representación gráfica das deteccións, agora compre ver como é que se 
        remarcan os obxectos detectados no vídeo. Isto faise mediante o elemento $<canvas>$ con
        id=``objects-canvas'' no que o observador da clase DetectedObjectsObserver encargase de debuxar
        un recadro da cor axeitada. Para coñecer esta cor que depende das opcións do panel o observador
        consulta a cada obxecto Detection empregando o método getCurrentColor() que se pode ver no 
        diagrama \ref{fig:DetectionClass}.

        
        \begin{figure}[htp]
        \begin{center}
            \includegraphics[scale=0.5]{figures/DetectionClass.pdf}
            \caption{Diagrama completo da clase Detection}
        \label{fig:DetectionClass}
        \end{center}
        \end{figure}
        
        Tamén cabe destacar aquí a función do observador TrainingMsgObserver, que é o encargado de 
        mostrar a etiqueta de Fotogramas de Adestramento e o recadro de cor amarelo nos primeiros 
        fotogramas do vídeo que corresponden aos empregados polo sistema de recoñecemento para obter o 
        fondo da imaxe e así poder distinguir os obxectos que se moven sobre el.
        
\section{v0.4 Arranxar bug's e mellorar a estratexia de probas}
    
    Esta versión estivo adicada a mellorar algúns aspectos transversais da aplicación que se 
    consideraban de importancia como a verificación e securización das páxinas web's que se detalla 
    con detalle no capítulo de Validación.
    
    \subsection{Re-analizar vídeo}
    En este sentido representa un incremento funcional 
        pequeno en comparación co acometido en outras iteracións, a funcionalidade mais destacable
        das elaboradas neste período foi a de Reanalizar o vídeo unha vez subido dende a páxina de 
        reprodución. Isto é moi útil no caso de que queiramos aplicar cambios ao sistema de 
        recoñecemento e ver velozmente como estes cambios afectan ao resultado da análise, xa que non é
        preciso volver a subir o vídeo á plataforma. O aspecto final desta funcionalidade pódese 
        ver na figura \ref{fig:reloadAnalysis}.
    
        \begin{figure}[htp]
        \begin{center}
            \includegraphics[scale=0.45]{figures/reloadAnalysis.png}
            \caption{Capturas de pantalla que amosan os distintos estados do botón de re-analise}
        \label{fig:reloadAnalysis}
        \end{center}
        \end{figure}

\section{v0.5 Traxectorias}
    As traxectorias son outra parte fundamental do sistema, debese mostrar a ruta seguida por 
    cada un dos obxectos detectados dende o momento no que entra en escena ata a súa saída. Para 
    que as traxectorias sexan detectadas no sistema de recoñecemento hai que invocar á biblioteca 
    de análise de alto nivel que require de unha cantidade de tempo considerable. É por elo que na 
    interface de liña de comando precisase un argumento de frecuencia que indique cada cantos 
    fotogramas se realizará a análise de alto nivel(por defecto cada 5 fotogramas).
    
    As traxectorias chegan á capa cliente como parte do XML resultado do sistema de análise, para 
    mostralas empregarase unha nova capa canvas en conxunto cun novo observador 
    TrajectoriesObserver que itera sobre as deteccións actuais pintando as liñas entre cada un dos
    puntos da súa ruta, dende o comezo desta ate o punto no que se atopa actualmente o obxecto. 
    Estes puntos son parseados polo obxecto Detection nada mais recibir o documento XML, quedando
    almacenados como parte do seu campo trajectory.
    
    Xa que a traxectoria forma parte da análise de alto nivel, e esta 
    non é executada en tódolos fotogramas, unha traxectoria estará composta por puntos que indican a
    posición do obxecto detectado con certa frecuencia (figura \ref{fig:calcTrajectory}).
    Para que entre un punto de detección e o seu
    seguinte pareza que hai avance, dando así sensación de fluidez, o que se fai é calcular dado o
    seguinte punto e o momento actual, cal é o lugar no que debería estar dito obxecto se se movese
    en liña recta entre ambos puntos, debuxando a liña da traxectoria ate esa posición calculada.
    
    \begin{figure}[htp]
    \begin{center}
        \includegraphics[scale=0.4]{figures/calcTrajectory.pdf}
        \caption{Diagrama do cálculo da traxectoria}
    \label{fig:calcTrajectory}
    \end{center}
    \end{figure}
    
    O cálculo realizase en base ao ``ratio'' que indica a distancia de entre os dous puntos que o coche 
    leva percorrido.
    
    \begin{verbatim}
        var ratio = Math.abs((nFrame - f1) / (f1 - f2));
        var x = p1.x + ratio * (p2.x - p1.x);
        var y = p1.y + ratio * (p2.y - p1.y);    
    \end{verbatim}
    
    Os resultados destes cálculos gárdanse para ser empregados en frames posteriores en caso de que
    sexa preciso.\\
    
    A parte das traxectorias, nesta iteración tamén se derou solución a outras funcionalidades como
    a támoa de Deteccións e a lista de deteccións actuais.

    \subsection{Táboa de Deteccións}

        Co fin de poder ollar todas as deteccións e podelas ordenar por orde de aparición, tempo en 
        escena, etc, crease unha táboa de obxectos detectados que ademais indica os obxectos que están
        a aparecer agora mesmo no vídeo, vexase figura \ref{fig:DetectionsTable}.
        A estes obxectos asígnaselle unha cor en función do seu 
        identificador unívoco, que ven xa composto por tres valores entre 0 e 255, e que se trata 
        segundo o model RGB como se mostra na figura \ref{fig:lightColor} para obter cores mais 
        brillantes e vistosas.
        
        \begin{figure}[htp]
        \begin{center}
            \includegraphics[scale=0.5]{figures/lightColor.pdf}
            \caption{Diagrama que mostra o aclarado de cores}
        \label{fig:lightColor}
        \end{center}
        \end{figure}

        Para o ordenamento da táboa empregouse a biblioteca javascript tablesorter que permite ordenar
        ascendente ou descendentemente a táboa facendo click na cabeceira do elemento polo que se desexa
        ordenar. Tamén é de destacar que os identificadores situados na primeira columna conteñen un 
        enlace que ao facer click leva o vídeo ao momento no que ese obxecto aparece por primeira vez en
        escena, función de tremenda utilidade á hora de recoñecelo.
        
        \begin{figure}[htp]
        \begin{center}
            \includegraphics[scale=0.4]{figures/DetectionsTable.png}
            \caption{Captura de pantalla que amosa a táboa de obxectos detectados}
        \label{fig:DetectionsTable}
        \end{center}
        \end{figure}
        
        Como é lóxico esta táboa actualizarase en cada un dos fotogramas se hai algún novo obxecto 
        detectado ou se algún de eles desaparece, marcando ou desmarcando este como obxectos en escena.
        Este comportamento implementase mediante dúas listas no obxecto VideoDetections: 
        \textbf{detRecentlyDeleted} e \textbf{detRecentlySelected}, que son lidas polo observador 
        DetectionsTableObserver, encargado de remarcar todas as Deteccións da lista detRecentlySelected
        e de desmarcar todas as da lista detRecentlyDeleted, estas relacións entre as clases 
        VideoDetections e Detection pódense observar no diagrama \ref{fig:Detection-VideoDetections}.
        
        \begin{figure}[htp]
        \begin{center}
            \includegraphics[scale=1]{figures/Detection-VideoDetections.pdf}
            \caption{Diagrama de clases que amosa detalladamente a relación entre as clases Detection
            e VideoDetections}
        \label{fig:Detection-VideoDetections}
        \end{center}
        \end{figure}
        
    \subsection{Lista de Deteccións actuais}

        Probablemente a función mais necesaria e vistosa é a de amosar os obxectos detectados de forma 
        individualizada mentres estes están en pantalla. A estes efectos crease unha lista de deteccións
        actuais que amosa unha imaxe de tódolos obxectos presentes no vídeo. O resultado pódese observar
        na figura \ref{fig:detectedObjects}.
        
        \begin{figure}[htp]
        \begin{center}
            \includegraphics[scale=0.4]{figures/detectedObjects.png}
            \caption{Captura de pantalla que amosa a lista de obxectos detectados}
        \label{fig:detectedObjects}
        \end{center}
        \end{figure}
    
        Esta sección de deteccións actuais ao igual que a táboa de obxectos detectados precisa ser 
        actualizada en cada fotograma, para elo empréganse as listas de detRecentlyDeleted e 
        detRecentlySelected explicadas anteriormente, só que agora é o observador 
        CurrentDetectionsObserver o encargado de engadir ou borrar unha detección desta sección.
        
        Tamén hai que destacar que xa que os observadores CurrentDetectionsObserver e 
        DetectionsTableObserver son os que mais tempo de execución consumen, a súa execución é 
        complementaria dependendo do estado do checkbox ``Show all detections''. De estar
        deshabilitado mantén o observador DetectionsTableObserver inactivo mentres que o 
        CurrentDetectionsObserver está activo, e estando habilitado fai o contrario. Isto obriga aos
        observadores a reimplementar os métodos ``enable()'' e ``disable()'', cargando ou
        descargando nestes métodos todos os valores actuais das deteccións que non actualizaron mentres
        estaban deshabilitados.
        
        As imaxes que se amosan como icona de cada un dos obxectos detectados son calculadas en tempo de
        reprodución do vídeo, isto é posible grazas aos elementos data URI\cite{data-uris} de javascript,
        que permiten almacenar unha imaxe como se fose unha URL e empregar esta URL para logo mostrar a 
        imaxe, por poñer o caso, no fondo (background-image) de un elemento HTML. Esta URI calcúlase 
        no método da clase VideoDetections ``captureImg'' empregando unha chamada a ``canvas.toDataURL()''
        onde canvas é un elemento HTMLCanvasElement obtido a partir do fotograma actual do vídeo.
        
        Por último para mostrar a información asociada a este obxecto detectado de algún xeito, 
        empregase a compoñente da librería bootstrap popover, que é unha ventá flotante que xurde ao
        carón da detección cando o rato pasa por enriba dela. Este popover personalizase coa cor 
        concreta desa detección (ver figura \ref{fig:popoverCapture}).
        
        \begin{figure}[htp]
        \begin{center}
            \includegraphics[scale=0.5]{figures/popoverCapture.png}
            \caption{Captura de pantalla que amosa o popover da lista de deteccións actuais}
        \label{fig:popoverCapture}
        \end{center}
        \end{figure}
        
    
\section{v0.6 Detección do comportamento anómalo}
    
    Un dos factores nos que destacan os algoritmos dos laboratorios VARPA é precisamente a análise
    de comportamento co fin de detectar o comportamento anormal de un dos obxectos implicados nunha
    escena. Isto faise calculando cal é o camiño polo que un obxecto soe viaxar do punto A ao 
    punto B, para calquera que sexan os puntos A e B, de xeito que podemos medir o anormal que é
    o seu comportamento en función do que o seu camiño se diferencie do camiño habitual. Isto pode
    axudar a detectar por exemplo un peón que non cruza polo paso de cebra, un coche facendo unha 
    manobra perigosa...
    
    O sistema de recoñecemento facilitado polo laboratorio foi deseñado para poder devolver un ratio
    de esa anormalidade en cada unha das chamadas ao sistema de análise de Alto nivel, que como se 
    dixo anteriormente executase cada determinado período de tempo. E dado que é este sistema o que
    proporciona a traxectoria do obxecto, o ratio de anormalidade para cada un dos momentos 
    asociouse pois a cada un dos puntos da traxectoria como se pode ver na imaxe \ref{fig:abXmlRate}.
    Como se pode comprobar nesta imaxe, existen unha serie de momentos iniciais nos que o ratio inda
    non puido ser calculado, e neste caso o valor asociado será 0.
    
    \begin{figure}[htp]
    \begin{center}
        \includegraphics[scale=0.5]{figures/abXmlRate.png}
        \caption{Ratio de anormalidade gardado nun documento XML}
    \label{fig:abXmlRate}
    \end{center}
    \end{figure}
    
    Non obstante un número é algo moi pouco gráfico, que dificilmente pode axudar a resaltar unha 
    detección anómala de unha que non o é, por este motivo se deseñou unha serie de compoñentes que
    permiten seleccionar un límite (un ratio de anormalidade máximo) a partir do cal un obxecto será
    considerado sospeitoso. A nivel de interface web, este límite pode marcarse cunha barra 
    selectora chamada slider, que  se importou da libraría jQuery UI\cite{ComponenteSliderJqueryUi},
    e que se complementou cunha entrada de texto para dar a posibilidade de que o valor se seleccione
    ou ben escribíndoo, ou ben a través do slider (vexase figura \ref{fig:sliderCapture}).
    Os valores a seleccionar variarán sempre entre 0 e o máximo valor atopado entre os ratios de
    anormalidade das deteccións.

    \begin{figure}[htp]
    \begin{center}
        \includegraphics[scale=0.5]{figures/sliderCapture.png}
        \caption{Captura de pantalla do compoñente slider da biblioteca jQuery UI}
    \label{fig:sliderCapture}
    \end{center}
    \end{figure}
    
    Agora que xa se pode seleccionar o valor a partir do cal queremos marcar unha detección como
    anómala, falemos do xeito no que se vaia realizar este marcado. A idea xeral consiste en 
    inabilitar o uso de diferentes cores e marcar de azul todas as deteccións, con excepción de 
    aquelas que teñan un comportamento anormal que serán marcadas de vermello, e aquelas cuxo 
    comportamento inda non fose analizado que serán marcada en cor negra como se pode ver na imaxe
    \ref{fig:suspiciousDetections}. Este uso de cores no só
    se emprega nas capas $<canvas>$ de información que se amosan sobre o vídeo senón tamén na táboa
    de obxectos detectados e na lista de deteccións actuais.

    \begin{figure}[htp]
    \begin{center}
        \includegraphics[scale=0.4]{figures/suspiciousDetections.png}
        \caption{Captura de pantalla que amosa o comportamento das deteccións sospeitosas}
    \label{fig:suspiciousDetections}
    \end{center}
    \end{figure}
    
    Para conseguir isto, centralizase a xestión da cor sobre o obxecto Detection que se pode ver en
    detalle no diagrama \ref{fig:DetectionClass}, facendo que sexa 
    este o que determine de forma unívoca de que cor se ten que debuxar. Para isto creanse os 
    métodos ``getCurrentColor'' que devolve a cor para as capas $<canvas>$ e 
    ``getCurrentLightColor'' que devolve un color mais claro para a táboa e a lista de deteccións.
    O obxecto Detection á súa vez calculara a cor a devolver en función do seu ratio de anormalidade
    e o valor das propiedade: videoDetections.alarmAbnormalRate, videoDetections.useAbnormalityRate
    e videoDetections.useColors.
    
    A parte deste mecanismo para centralizar a xestión do color que será empregado por tódolos 
    observadores, os observadores DetectionsTableObserver e CurrentDetectionsObserver precisan 
    coñecer aquelas deteccións que cambiaron a un estado sospeitoso, de confianza ou sen datos,
    con este obxectivo crease a lista VideoDetections.abChangingDetections (ver diagrama 
    \ref{fig:Detection-VideoDetections}), que indicará aquelas
    deteccións que cambiaron de estado e polo tanto, no caso da táboa e a lista de deteccións, 
    precisan mudar a súa cor.
    
    \subsection{Popups de deteccións sospeitosas}

        Para completar a funcionalidade de detectar o comportamento anómalo, solicitase unha faceta a 
        maiores do marcado das deteccións sospeitosas, deberase dar a opción de lanzar unha nova ventá
        por cada obxecto sospeitoso detectado coa posibilidade de ver este obxecto de preto ao largo do
        seu percorrido.
        
        Tras estudar varios xeito de facer isto, a mellor solución parece ser a de empregar pop-ups 
        tamén chamados ventás emerxentes, que inda que normalmente se evitan por estares asociados 
        con contido publicitario, neste caso compren á perfección coa tarefa que se desexa realizar.
        
        Xa que haberá que ampliar a imaxe facendo un efecto zoom para poder mostrar a zoa do vídeo na
        que está a detección con mais detalle, prantexase a construción dun sistema que mediante unha
        barra selectora (slider) permita achegarse ou afastarse para ver mais de preto ou de lonxe o 
        obxecto detectado. Para estas ventás emerxentes empregarase un novo ficheiro .css e tamén un 
        novo ficheiro .js, que conterá os mecanismos precisos para crear este efecto zoom.
        
        A esta nova ventá pasaranse como parámetros da URL o identificador da detección e a ULR do 
        ficheiro XML que contén a análise. Con estes datos o ficheiro javascript principal chamado
        suspicious-popup.js, que segue unha distribución similar ao de video-player.js para a páxina
        que amosa o vídeo, obterá o ficheiro XML e en base a el iniciará o sistema. Esta
        páxina tamén constará dun elemento $<video>$, con capas $<canvas>$ superpostas, a primeira delas 
        ocultará o vídeo por completo mostrando a porción del que sexa precisa para lograr o efecto 
        zoom e as sucesivas mostrarán a información desexada.
        
        Ao manter a estrutura dun elemento $<video>$ coas capas $<canvas>$ superpostas, a mellor 
        arquitectura posible e a dun patrón Observador, no que o suxeito manteña ademais das referencias
        ao vídeo, os datos sobre o nivel de zoom, que será necesariamente empregado polos seus 
        observadores. Para esta tarefa reempregouse gran parte do deseño do diagrama \ref{fig:observerPattern}
        engadindo as características precisas nunha clase herdada de VideoDetections que se nomea 
        ZoomVideoDetections como se pode ver na figura \ref{fig:SuspiciousPopupJS}.
        
        \begin{figure}[htp]
        \begin{center}
            \includegraphics[scale=0.45]{figures/SuspiciousPopupJS.pdf}
            \caption{Diagrama de clases simplificado para a ventá emerxente de deteccións sospeitosas}
        \label{fig:SuspiciousPopupJS}
        \end{center}
        \end{figure} 
        
        Este sistema é iniciado na carga do XML e ao igual que na páxina que reproduce o vídeo, cando a 
        carga finaliza crease
        un bucle coa función updateStatus que actualizará en cada fotograma o estado dos compoñentes 
        chamando á función ZoomVideoDetections.updateState(). Esta función tamén é chamada cando o vídeo
        lanza o evento play, ou ao cambiar o nivel de zoom mediante a barra selectora (slider).
        O resultado final, mostrase quedou como se pode ver na figura \ref{fig:suspiciousPopupCapture}.

        \begin{figure}[htp]
        \begin{center}
            \includegraphics[scale=0.4]{figures/suspiciousPopupCapture.png}
            \caption{Captura de pantalla que amosa a ventá de deteccións sospeitosas}
        \label{fig:suspiciousPopupCapture}
        \end{center}
        \end{figure} 
        
        A continuación explicase en detalle o mecanismo empregado para crear o efecto zoom e para 
        levar a cabo o seguimento do obxecto mantendo este nivel de zoom. Empréganse 
        basicamente dúas variables, por un lado o tamaño da fracción de vídeo a amosar (w\_size), que
        variará entre un mínimo(dúas veces o tamaño da detección) e un máximo que é o tamaño do vídeo, 
        e por outro lado o centro (center) da escena que será unha variable composta de dúas 
        compoñentes (x,y) que indican a distancia en píxeles á parte superior esquerda do vídeo.
        
        A función ZoomVideoDetections.centerToLeftTop é a encargada de calcular dado un tamaño de ventá 
        e un centro, esta distancia á marxe esquerda superior do vídeo. Este cálculo no é sinxelo, xa que 
        hai que evitar que o recadro a seleccionar exceda das dimensións do vídeo. Esta casuística 
        amosase na figura \ref{fig:centerToLeftTop}.
        
        \begin{figure}[htp]
        \begin{center}
            \includegraphics[scale=0.75]{figures/centerToLeftTop.pdf}
            \caption{Diagrama que explica os problemas resoltos para implementar o efecto zoom}
        \label{fig:centerToLeftTop}
        \end{center}
        \end{figure}

\section{v1.0 Memoria e posta en produción}
    Esta última fase adicouse principalmente a completar a memoria, a solucionar pequenos problemas
    acumulados nas anteriores versións e á publicación de esta aplicación baixo un entorno de 
    produción.
    
    Para a publicación desta aplicación web, realizarase a súa montaxe sobre un servidor on-line,
    é dicir, unha máquina conectada a internet que dispón de conexión directa a unha dirección IP
    fixa, e a capacidade para manterse en funcionamento de forma continua. A esta IP referenciará un
    servidor DNS, que se contratará xunto co dominio \url{www.ancoweb.es} a través do cal poderase
    acceder á páxina.

    \subsection{Dominio}
        O dominio ten que ser rexistrado cos datos da persoa responsable, neste caso o autor do 
        proxecto, a través dunha empresa rexistradora. Seleccionase a empresa 1\&1
        \cite{1and1-website} xa que tamén oferta servizo DNS(Domain Name System) para apuntar ao 
        servidor que contén a aplicación.
        
        Unha vez creado o dominio, a única configuración a establecer para que este funcione 
        é a dirección do servidor, que indicaremos no rexistro de tipo A como se pode
        ver na figura \ref{fig:1and1Capture} onde a dirección IP do servidor é 45.55.51.164.

        \begin{figure}[htp]
        \begin{center}
            \includegraphics[scale=0.5]{figures/1and1Capture.png}
            \caption{Panel de control da ferramenta 1and1}
        \label{fig:1and1Capture}
        \end{center}
        \end{figure}
    
    \subsection{Hosting}
        Na dirección IP 45.55.51.164 reside o servidor montado para distribuír a aplicación web a 
        todo aquel que se conecte. Este servidor pertence á empresa DigitalOcean
        \cite{digitalocean-website} que se seleccionou de entre outros servizos de hosting por 
        permitir a posibilidade de crear o entorno de produción dende zero, podendo así instalar 
        todos os paquetes preciso para a integración das múltiples tecnoloxías implicadas neste 
        proxecto.
        
        Neste caso seleccionouse como punto de partida unha máquina Ubuntu Server 14.04 con 1GB de 
        memoria RAM, un só núcleo de CPU e 30GB de memoria SSD, todo elo localizado en Nova York.
    
    \subsection{Configuración do Servidor}
        A partir da base que nos proporciona DigitalOcean, que é unha máquina Ubuntu Server 14.04 
        completamente limpa, pódese seguir a guía contida no ficheiro production-README.md situado
        na base do proxecto que indica con detalle todos os comandos a executar para una correcta 
        configuración. 
        
        Como peza central escolleuse o servidor Apache2, este servidor será o encargado de servir 
        tódolos arquivos, tanto páxinas web como ficheiros multimedia e recursos da aplicación. Mais
        este servidor de por si só, non pode executar o framework de Django que se precisa para 
        atender as peticións dos clientes, co que se engade un módulo chamado mod\_wsgi
        que permite executar código python sobre o servidor mediante o ficheiro do proxecto
        ``src/ancoweb/wsgi.py''. A dificultade especia está en configurar adecuadamente todo o entorno
        para executar Python 3.4, pois é unha versión soportada por mod\_wsgi recentemente.
        
    \subsection{Axustes de Produción}
        En produción existen toda unha serie de parámetros que cambian en relación co entorno de 
        desenvolvemento, para soportalos creouse un novo ficheiro de configuración que herda do 
        anterior chamado ``settings\_production.py'' e que está situado no directorio 
        ``/src/ancoweb''.
        
        Para indicar ao sistema que empregue este ficheiro en lugar de settings.py, o que se fixo 
        foi modificar o ficheiro ``src/ancoweb/wsgi.py'' facendo que este apunte ao ficheiro de 
        produción en lugar de ao de desenvolvemento, así cada vez que o proxecto se execute mediante
        mod\_wsgi empregaranse os axustes de produción, e en caso contrario os axustes por defecto.
        
        Un dos axustes de configuración mais importantes é o da variable STATIC\_ROOT, esta indica
        onde se almacenarán os recursos da aplicación, e neste caso decidiuse empregar o directorio
        ``/var/www/static/'' polo que antes de ser despregada todos o recursos precisos de tódalas 
        aplicacións incluídas deben ser movidos a este directorio empregando o comando:
        
        \begin{verbatim}
        sudo python manage.py collectstatic 
            --settings=ancoweb.settings_production
        \end{verbatim}

        O outro axuste de gran interese é o lugar no que se almacenan os ficheiros multimedia, pois
        a meirande parte da aplicación xira en torno a eles. Neste caso a variable en conflito é 
        MEDIA\_ROOT, que no caso do entorno de produción tomará o valor ``/var/www/media/'', e en 
        caso de estar en desenvolvemento ``MEDIA\_ROOT = str(BASE\_DIR / 'media/')''. Isto obriga ao 
        igual que cos recursos do proxecto a mover os arquivos multimedia cada vez que se desexe 
        despregar a aplicación, neste caso co comando:
        
        \begin{verbatim}
        sudo cp -R ancoweb-TFG/src/media/ /var/www
        \end{verbatim}
